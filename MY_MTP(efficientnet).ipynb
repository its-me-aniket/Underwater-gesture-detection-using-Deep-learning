{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13046017,"sourceType":"datasetVersion","datasetId":8261109},{"sourceId":13049472,"sourceType":"datasetVersion","datasetId":8263469}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Underwater Hand Gesture Classification with EfficientNet\n\nThis Colab notebook trains an **EfficientNet-B0 CNN** on underwater hand gesture images.\n\nSteps:\n1. Mount Google Drive & prepare dataset\n2. Create stratified train/val/test splits\n3. Build PyTorch datasets & dataloaders\n4. Apply image augmentations\n5. Train EfficientNet\n6. Validate with confusion matrix & per-class accuracy\n7. Test on held-out set","metadata":{}},{"cell_type":"markdown","source":"## Step 1 - Mount Google Drive and Split Dataset","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"aniket2125/underwater-hand-gesture-images\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:46:23.303069Z","iopub.execute_input":"2025-09-13T17:46:23.303310Z","iopub.status.idle":"2025-09-13T17:46:23.376449Z","shell.execute_reply.started":"2025-09-13T17:46:23.303285Z","shell.execute_reply":"2025-09-13T17:46:23.375840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n# Dataset path \ndataset_dir = os.path.join('/kaggle/input', 'underwater-hand-gesture-images')\nassert os.path.isdir(dataset_dir), f\"dataset_dir not found: {dataset_dir}\"\n\n# Load dataset to inspect classes\nfull = ImageFolder(root=dataset_dir)\nclasses = full.classes\nprint(\"Detected classes:\", classes)\nprint(\"Num images:\", len(full))\nprint(\"Per-class counts:\", Counter(full.targets))\n\n# Stratified split (70/15/15)\nindices = np.arange(len(full))\ntargets = np.array(full.targets)\n\ntrain_idx, temp_idx, y_train, y_temp = train_test_split(\n    indices, targets, test_size=0.30, stratify=targets, random_state=42\n)\nval_idx, test_idx, y_val, y_test = train_test_split(\n    temp_idx, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n)\n\nprint(f\"Train: {len(train_idx)} | Val: {len(val_idx)} | Test: {len(test_idx)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:49:38.840460Z","iopub.execute_input":"2025-09-13T17:49:38.840727Z","iopub.status.idle":"2025-09-13T17:49:42.007901Z","shell.execute_reply.started":"2025-09-13T17:49:38.840707Z","shell.execute_reply":"2025-09-13T17:49:42.007267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2 - Create PyTorch Datasets and Dataloaders","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom torch.utils.data import Subset, DataLoader\n\n# Placeholder transforms (will be updated in Step 3)\nnormalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\nbasic_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    normalize,\n])\n\n# Datasets with Subset\ntrain_dataset_full = ImageFolder(root=dataset_dir, transform=basic_transform)\nval_dataset_full   = ImageFolder(root=dataset_dir, transform=basic_transform)\ntest_dataset_full  = ImageFolder(root=dataset_dir, transform=basic_transform)\n\ntrain_dataset = Subset(train_dataset_full, train_idx)\nval_dataset   = Subset(val_dataset_full, val_idx)\ntest_dataset  = Subset(test_dataset_full, test_idx)\n\n# Dataloaders\nbatch_size = 32\nnum_workers = 2\npin_memory = True if torch.cuda.is_available() else False\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                          num_workers=num_workers, pin_memory=pin_memory)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n                        num_workers=num_workers, pin_memory=pin_memory)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                         num_workers=num_workers, pin_memory=pin_memory)\n\nprint(\"Example batch:\")\nimgs, lbls = next(iter(train_loader))\nprint(imgs.shape, lbls.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:49:56.900398Z","iopub.execute_input":"2025-09-13T17:49:56.901101Z","iopub.status.idle":"2025-09-13T17:50:00.915056Z","shell.execute_reply.started":"2025-09-13T17:49:56.901076Z","shell.execute_reply":"2025-09-13T17:50:00.914247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3 - Define Data Augmentation","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\nnormalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n    transforms.ToTensor(),\n    normalize\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Re-create datasets with new transforms\ntrain_ds_full = ImageFolder(root=dataset_dir, transform=train_transform)\nval_ds_full   = ImageFolder(root=dataset_dir, transform=val_transform)\ntest_ds_full  = ImageFolder(root=dataset_dir, transform=val_transform)\n\ntrain_dataset = Subset(train_ds_full, train_idx)\nval_dataset   = Subset(val_ds_full, val_idx)\ntest_dataset  = Subset(test_ds_full, test_idx)\n\nprint(\"Transforms applied. Sample:\")\nx, y = train_dataset[0]\nprint(x.shape, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:50:11.962013Z","iopub.execute_input":"2025-09-13T17:50:11.962849Z","iopub.status.idle":"2025-09-13T17:50:14.989453Z","shell.execute_reply.started":"2025-09-13T17:50:11.962819Z","shell.execute_reply":"2025-09-13T17:50:14.988678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4 - Load EfficientNet-B0 and Training Loop","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.amp import GradScaler, autocast\nimport time\n\nfull = ImageFolder(dataset_dir)\nclass_names = full.classes\nnum_classes = len(class_names)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Device:\", device)\n\nweights_path = \"/kaggle/input/efficientnet-b0-weights/efficientnet_b0_rwightman-7f5810bc.pth\"\nstate_dict = torch.load(weights_path, map_location=device)\nmodel = models.efficientnet_b0(weights=None)\nmodel.load_state_dict(state_dict)\nin_features = model.classifier[1].in_features\nmodel.classifier[1] = nn.Linear(in_features, num_classes)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\nscaler = GradScaler()\n\ndef train_one_epoch(model, loader, optimizer, criterion, device, scaler):\n    model.train()\n    running_loss, preds_all, labels_all = 0.0, [], []\n    for imgs, labels in loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        with autocast(device_type=\"cuda\" if device == \"cuda\" else \"cpu\"):\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item() * imgs.size(0)\n        preds_all.extend(outputs.argmax(1).cpu().numpy())\n        labels_all.extend(labels.cpu().numpy())\n    return running_loss/len(loader.dataset), (np.array(preds_all)==np.array(labels_all)).mean()\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss, preds_all, labels_all = 0.0, [], []\n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * imgs.size(0)\n            preds_all.extend(outputs.argmax(1).cpu().numpy())\n            labels_all.extend(labels.cpu().numpy())\n    preds_all, labels_all = np.array(preds_all), np.array(labels_all)\n    return running_loss/len(loader.dataset), (preds_all==labels_all).mean(), labels_all, preds_all\n\nnum_epochs = 12\nbest_val_acc = 0.0\nckpt_path = 'best_efficientnet_b0.pth'\n\nfor epoch in range(1, num_epochs+1):\n    t0 = time.time()\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n    val_loss, val_acc, y_true_val, y_pred_val = validate(model, val_loader, criterion, device)\n    scheduler.step(val_loss)\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(), 'val_acc': val_acc}, ckpt_path)\n    print(f\"Epoch {epoch}/{num_epochs}: train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n          f\"val_loss={val_loss:.4f} acc={val_acc:.4f} time={time.time()-t0:.1f}s\")\nprint(\"Best Val Acc:\", best_val_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:03:32.939965Z","iopub.execute_input":"2025-09-13T18:03:32.940334Z","iopub.status.idle":"2025-09-13T18:09:49.716836Z","shell.execute_reply.started":"2025-09-13T18:03:32.940310Z","shell.execute_reply":"2025-09-13T18:09:49.715932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5 - Validation Confusion Matrix and Per-class Accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\n\nval_loss, val_acc, y_true_val, y_pred_val = validate(model, val_loader, criterion, device)\n\nprint(\"Validation accuracy:\", val_acc)\ncm = confusion_matrix(y_true_val, y_pred_val)\nprint(\"Confusion matrix:\\n\", cm)\nper_class_acc = np.diag(cm) / cm.sum(axis=1)\nfor cls, acc in zip(class_names, per_class_acc):\n    print(f\"{cls}: {acc*100:.2f}%\")\nprint(classification_report(y_true_val, y_pred_val, target_names=class_names))\n\ncm_norm = cm.astype(float)/cm.sum(axis=1)[:,None]\nfig, ax = plt.subplots(figsize=(7,6))\nim = ax.imshow(cm_norm, cmap='Blues')\nplt.colorbar(im)\nax.set_xticks(range(len(class_names)))\nax.set_yticks(range(len(class_names)))\nax.set_xticklabels(class_names, rotation=45)\nax.set_yticklabels(class_names)\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(j,i,f\"{cm_norm[i,j]:.2f}\",ha='center',va='center',\n                color='white' if cm_norm[i,j]>0.5 else 'black')\nplt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\nplt.title(\"Validation Confusion Matrix (normalized)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:10:19.048530Z","iopub.execute_input":"2025-09-13T18:10:19.049373Z","iopub.status.idle":"2025-09-13T18:10:22.839244Z","shell.execute_reply.started":"2025-09-13T18:10:19.049324Z","shell.execute_reply":"2025-09-13T18:10:22.838284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6 - Test Evaluation","metadata":{}},{"cell_type":"code","source":"if os.path.exists(ckpt_path):\n    ckpt = torch.load(ckpt_path, map_location=device, weights_only= False)\n    model.load_state_dict(ckpt['model_state_dict'])\n    print(\"Loaded checkpoint at epoch\", ckpt.get('epoch'), \"val_acc\", ckpt.get('val_acc'))\nelse:\n    print(\"Checkpoint not found, using current model weights.\")\n\ntest_loss, test_acc, y_true_test, y_pred_test = validate(model, test_loader, criterion, device)\nprint(\"Test Accuracy:\", test_acc)\n\ncm_test = confusion_matrix(y_true_test, y_pred_test)\nprint(\"Test confusion matrix:\\n\", cm_test)\nper_class_acc_test = np.diag(cm_test) / cm_test.sum(axis=1)\nfor cls, acc in zip(class_names, per_class_acc_test):\n    print(f\"{cls}: {acc*100:.2f}%\")\nprint(classification_report(y_true_test, y_pred_test, target_names=class_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:14:47.564695Z","iopub.execute_input":"2025-09-13T18:14:47.565000Z","iopub.status.idle":"2025-09-13T18:14:53.021164Z","shell.execute_reply.started":"2025-09-13T18:14:47.564980Z","shell.execute_reply":"2025-09-13T18:14:53.020271Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7 - Save Class Mapping and Model Utility","metadata":{}},{"cell_type":"code","source":"import json\n\nclass_map = {i:c for i,c in enumerate(class_names)}\nwith open('class_map.json','w') as f:\n    json.dump(class_map,f,indent=2)\nprint(\"Saved class map.\")\n\n# To reload model:\n# model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n# model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(class_names))\n# model.load_state_dict(torch.load('best_efficientnet_b0.pth')['model_state_dict'])\n# model.eval()\n","metadata":{},"outputs":[],"execution_count":null}]}